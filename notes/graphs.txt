Representations of graphs:
G = (V,E), where V is the set of vertices and E is the set of edges
1) adjacency list
  - This is the most common choice.
  - Provides a compact way to represent sparse graphs - those for which E is much less than V^2.
  - Consists of an array "Adj" of lists, one for each vertex in V. Each list u contains all the vertices v such that there is an edge (u,v) in E. That is, Adj[u] consists of all the vertices adjacent to u in G (or pointers to those vertices).
  - For a directed graph, the sum of the lengths of all the adjacency lists is E. For undirected graphs it is 2E (since edges can be said to go both ways). 
  - Requires O(V+E) memory for both directed and undirected graphs. 
  - Weighted graphs can be handled by storing the weight w(u,v) of the edge (u,v) with vertex v in u's adjacency list.
  - potential disadvantage: no quicker way to search for a particular edge (u,v) than to search for v in the adjacency list Adj[u]. An adjacency matrix (see below) remedies this at the cost of more memory.
2) adjacency matrix
  - Useful when the graph is dense - that is, E is close to V^2
  - Also useful is we need to tell quickly if there is an edge connexting 2 given vertices (for example, in 2 of the all-pairs-shortest-paths algorithms assume their input graphs are represented by adjacency matrices).
  - We assume all the vertices are numbered. Then, the matric representation of a graph consist of a V*V matrix A. An entry Aij is 1 if an edge exists between i and j.
  - Requires O(V^2) memory.
  - For an undirected graph, the adjacency matrix is its own transpose, since (u,v) and (v,u) represent the same edge. For this reason, we sometimes only store the entries on and above the diagonal to save memory.
  - To represent a weighted graph, can just store the weight value rather than 1.
  - Though more memory is required for an adjacency matrix than in an adjacency list, they are simpler, and require only 1 bit per entry (unless you're storing the weight rather than just yes/no).

-=- Breadth-first search
The archetype for many important graph algorithms. Prim's minimum-spanning-tree algo and Dijkstra's single-source shortest-path algo use similar ideas.
Starting with a source vertex s, breadth-first search discovers all vertices at distance k from s before discovering vertices at distance k+1.
BFS uses node colors and a Queue as a tool to keep track of traversal. 
  - At the start, all nodes are white except the "source" vertex, which is gray.
  - For each vertex connected to s, we:
    - turn it gray because it is now "discovered". 
    - Set its "parent" attribute to vertex s.
    - Set it's "distance" attribute to be 1 + that of its parent's (its distance from s)
    - Enqueue it so it is later processed just like s is being processed now. 
  - We then turn s from gray to black, having fully processed it.
  - While the queue has length, we continually pop off vertices and process them as above.
By adding the "parent" attribute to each vertex connected to s, the BFS algorithm produces a subgraph that we call a "breadth-first tree" (it is in fact a tree, since |Eπ| = |Vπ| - 1), which is itself a graph denoted by Gπ = (Vπ, Eπ) that contains all vertices reachable from s. 
The breadth-first tree records a shortest path between each vertex and s.
Runtime:
Each vertex is handled by the queue once, and when it is, we handle each one of its edges. Thus, the runtime of BFS is O(V+E), That is, it's linear with respect to the adjacency list-representation. 

-=- Depth-first search
DFS is often used as a subroutine in another algorithm (as we shall see later).
Works on directed and undirected graphs.
Search "deeper" in the graph whenever possible:
  - Explores edges out of the most recently discovered vertex v that still has unexplored edges leaving it. 
  - Whenever a vertex v is discovered, we set v.P to the vertex from which is is discovered (as in BFS).
  - Once all of v's edges are explored, the seaerch "backtracks" to explore edges from the vertex from which v discovered. 
  - This continues until we have reached all vertices reachable from the original source vertex. 
  - If any undiscovered vertices remain, we repeat the process for them until every vertex has been discovered.
  - Like BFS, we use the colors to keep track of where we've been, so each node only gets added to 1 tree.
  - Each vertex also gets 2 timestamps (time is kept globally by an incrementing integer): one marking the time when v is discovered (grayed), and once when it is finished (blackened). Timestamps are between 1 and 2V, since each vertex is stamped twice.
Unlike BFS, the predecessor graph generated by DFS may consist of several trees, since the search may repeat from ultiple sources. We define this predecessor graph as:
  Gπ = (V, Eπ), where 
  Gπ is a "depth-first forest" comprised of several "depth-first trees", and
  the edges in Eπ are the tree edges.
The exact graph resulting from running DFS will vary based on the order of the nodes processed, and the order of edges recorded in the adjacency-list. This generally does not matter.
Runtime:
We call the helper DFSVisit once on each vertex (O(V)), and the function loops through all the vertices adjacent to that vertex, only calling itself again for vertices that are still white (O(E)). Thus, runtime for DFS, like BFS, is O(V+E).

Properties of DFS
- The structure of the trees in the resulting forest exactly mirrors the structure of the recursive calls of DFSVisit.
- Theorem: The processing of nodes form a parenthetical structure. That is, for any 2 vertices u and v, exactly 1 of the following conditions is true:
  1. The intervals [u.start, u.end] and [v.start, v.end] are disjoint, and neither u nor v is a descendant of the other in the depth-first forest.
  2. [u.start, u.end] is entirely contained within [v.start, v.end], and u is a descendant of v in a depth-first tree.
  3. [v.start, v.end] is entirely contained within [u.start, u.end], and v is a descendant of u in a depth-first tree.
- Theorem: vertex v is a proper descendant of vertex u in the depth-first forest for a (directed or undirected) graph if and only if u.start < v.start < v.end < u.end.
- "White path theorem": In a depth-first forest of a (directed or undirected) graph, vertex v is a descendent of vertex u if and only if at the time u.start, there is a path connecting u to v consisting of only white vertices.

Classification of edges
DFS can be used to classify edges of an input graph, and the type of edge can provide info about the graph. For example, we can tell that a directed graph is acyclic if and only if a depth-first search yields no "back" edges (since that would mean the descendant has a path to the ancestor that is different form the path DFS has already taken from the ancestor to the descendant).
4 types of edges:
  1) "Tree edges" are edges in the depth-first forest Gπ. Edge (u,v) is a tree edge if v was first discovered by exploring edge (u,v). (going down)
  2) "Back edges" are those edges (u,v) connected u to an ancestor v in a depth-first tree. This includes self-loops, which may occur in directed graphs. (ran out of white vertices, going back up)
  3) "Forward edges" are those nontree edges (u,v) connecting a vertex u to a descendent v in a depth-first tree. (not a tree edge, since the descendent is no longer white)
  4) "Cross edges" are all other edges. They can go between vertices in the same DF tree, as long as one vertex is not an ancestor of the other, or they can go between vertices in different DF trees.
During DFS, when we explore the edge (u,v), the color of v tells us something about the edge:
 - White indicates a tree edge
 - Gray indicates a back edge
 - Black indicates a forward or cross edge. The edge (u,v) is a forward edge if u.start < v.start, and a cross edge if u.start > v.start.
An undirected graph has ambiguity as to the type of each edge, since each edge occurs twice. Don't worry about it - whichever you encounter first.
Theorem: In a DFS of an undirected graph, every edge is either a tree edge or a back edge.

Topological sorting of a directed ascyclic graph (dag) using DFS
Topological sorting is a linear ordering of vertices such that for every directed edge (u,v), u comes before v in the odering. 
Topological sorting of a graph is not possible if the graph is not a dag.

-=- Strongly Connected Components
A classic application of DFS is decomposing a directed graph into its strongly connected components (SCCs). It is a common first step in many graph algos, and we can do it using 2 depth-first searches.
A strongly connected component of a graph is a maximal set of vertices such that every pair of vertices are reachable from each other.
Our algo for finding the SCCs of a graph G uses the transpose of G, which we define as GT=(V, ET), where ET consists of the edges of G with their directions reversed. The time to compute GT from the adjacency-list representation of G is O(V+E).
Note that G and GT have the same SCCs: vertices u and v are reachable from each other in G if and only if they are reachable from each other in GT. 
The following linear time O(V+E) algorithm computes the SCCs of a directed graph G=(V,E) using 2 depth-first searches, one on G and one on GT:
STRONGLY-CONNECTED-COMPONENTS(G)
  1. call DFS(G) to compute finishing times u.f for each vertex u
  2. compute GT
  3. call DFS(GT), but in the main loop of DFS, consider the vertices in order of decreasing u.f as computed in line 1 (that is, in topologically sorted order)
  4. output the vertices of each tree in the depth-first forest formed in step 3 as a separate SCC
The key property is that the component graph is a dag (if it was not acyclic, then we would not have distinct SCCs).

Why it works:
- Theorem: Let C and C' be distinct SSCs in a directed graph G=(V,E). Suppose there is an edge (u,v) in E, where u is in C and v is in C'. Then, f(C) > f(C'). That is, the max finishing time in the tree C is greater than that of C'. This is true regardless of whether any vertex in C is dicsovered by DFS before any vertex in C', or vice versa.
- Corrollary: The above theorem tells us that each edge in GT that goes between distinct SSCs, goes from a component with an earlier finishing time in the DFS to a component with later finishing time. That is, if there in an edge (u,v) in ET, where u in C and v in C', then f(C) < F(C').
- The above corrollary tells us why the SCC algo works: 
  - When we perform the 2nd DFS (on GT) in step 3, we start with some vertex in the SCC whose finishing time in the 1st DFS is greater than all the other SCCs. The search starts from some vertex in C, and visits all other vertices in C. The above corrollary tells us that if C contains no vertices that point to other SCCs. Thus, the resulting depth-first tree rooted at x contains the vertices in C and no others. 
  - The next white node y that the DFS in step 3 finds is a member of the SCC C' with the second greatest finish time of the SCCs. Vertices in C' only contain edges pointing to other vertices in C', and to C (but all vertices in C are no longer white). Thus, the resulting depth-first tree rooted at y contains the vertices in C' and no others. And so on for each SCC. 
  - Therefore, each highest level VISIT call in the 2nd DFS generates a depth-first tree representing an SCC.


-=-=- Minimum Spanning Trees
Example of the problem: We have n electric pins in a circuit that we want to connect. We can use an arrangement of n-1 wires, each connecting 2 pins. We want to find the configuration that uses the least amount if wire possible. For each edge (u,v) we have a weight, which is the cost of wire to connect pins u and v. We call the result a minimum spanning tree (it is a tree, since it's acyclic and connects all the vertices). 
We can solve the minimum spanning tree problem using Krushal's algo and Prim's algo (both greedy algorithms). Each are O(E lg V) using a binary heap. By using Fibonacci heaps, we can make Prim's algo run in O(E + V lg V), which improves performance if V is much smaller than E.

Useful definitions:
 - A "cut" (S, V - S) of an undirected graph G=(V,E) is a partition of V. 
 - We say an edge (u,v) "crosses" the cut if one of its endpoints is in S and the other is in V-S. 
 - We say that a cut "respects" a set A of edges if no edge in A crosses the cut. 
 - An edge is a "light edge" crossing a cut if its weight is the minimum of any edge crossing the cut. There can be >1 light edges in the case of ties.

  Theorem: Let:
  - G = (V, E) be a connected, undirected graph with a real valued weight function w defined on E,
  - A be a subset of E that is included in some miminim spanning tree for G, 
  - (S,V-S) be any cut of G that respects A, and
  - (u,v) be a light edge crossing (S,V-S).
Then edge (u,v) is safe for A. That is, if (u,v) is added to A, A will still be a subset of E that is included in a mimimum spanning tree for G.
  Corrollary: Let:
  - G = (V, E) be a connected, undirected graph with a real valued weight function w defined on E,
  - A be a subset of E that is included in some miminim spanning tree for G, 
  - C = (Vc, Ec) be a connected component (tree) in the forest Ga = (V, A).
If (u,v) is a light edge connecting C to some other component in Ga, then (u,v) is safe for A.
Proof: the cut (Vc, V-Vc) respects A, and (u,v) is a light edge for this cut. Therefore, (u,v) is safe for A.

-=- Kruskal's Algorithm
Finds a safe edge to add to the growing forest by finding, of all the dges that connect any 2 trees in the forest, an edge (u,v) of least weight. See programmed example.
Runtime: O(E lg V)  (the logic used to figure this out is kinda complex, see p.633)

-=- Prim's Algorithm
Has the property that the edges in the set A always form a single tree, which starts with an arbitrary root r, and grows until the tree spans all the vertices V. Each step adds to the tree A a light edge that connects A to an isoltaed vertex.
Runtime: O(E lg V) if using a binary heap, O(E + lg v) if using Fibonacci Heap (analysis on p. 636).


-=-=- Single-Source Shortest Paths (SSSP)
In a "shortest-paths" problem, we are given a weighted, directed graph G=(V,E), with weight function w : E -> R mapping edges to real-valued weights. The weight w(p) of path p = <v0,v1,...vk> is the sum of the weights of its constituent edges.
We denote the shortest path weight from vertex u to v with the symbol 𝛿(u,v). If there is no path from u to v, we say 𝛿(u,v)=∞. 
A shortest path from vertex u to vertex v is then defined as any path p with weight w(p) = 𝛿(u,v).
One great application is to model a roadmap, and determine the shortest distance between 2 intersections. However, the weights can also be used to represent other metrics than distance, such as time, cost, penalties, loss, or any other quantity that accumulates linearly along a path that we want to minimize.

Variants:
 - SSSP problem: Find a shortest path to a given destination vertex t from each vertex v.
 - Single-pair shortest-path problem: find a shortest path from u to v for given vertices u and v. If we solve the single-source problem, we solve this one (and the 2 have the same runtime).
 - All-pairs shortest-paths problem: Find a shortest path from u to v for every pair of vertices u and v.

Substructure
Shortest-paths algos typically rely on the property that a shortest path between 2 vertices contains other shortest paths within it.
 - Lemma: Given a weighted, directed graph G=(V,E) with weight function w: E -> R, let p=<v0, v1,...,vk> be a shortest path from vertex v0 to vk, and for any i and j such that 0<=i<=j<=k, let pij=<pi,pi+1,...,pj> be the subpath of p from vertex vi to vertex vj. Then, pij is a shortest path from vi to vj.

Negative weights
Some instances of SSSP problems may include edges with negative weights. So long as no negative weight cycles are reachable from source s, the shortest-path weight 𝛿(s,v) is well defined.

Cycles
We will have no negative weight cycles, positive weight cycles, or 0-weight cycles in our shortest paths (they are simple paths). Thus, any shortest path will traverse at most V distinct vertices, and V-1 edges.

Representation
We represent shortest paths similarly to how we did in BFS: with each vertex holding a pointer to the previous vertex along the path. 
PRINT-PATH(G,s,v) then prints out the path from source vertex s to vertex v. 
Like with BFS, we'll be concerned with the "predecessor subgraph" Gπ=(Vπ, Eπ), where Vπ is the set of vertices of G with non-nil predecessors, plus the source s.
The directed edge set Eπ is the set of edges induced by the π values for vertices in Vπ.
We will later prove that at termination, Gπ is a "shortest-paths tree": a rooted tree containing a shortest path from the source s to every vertex reachable from s.
Specifically, let G=(V,E) be a weighted, directed graph with no negative weight cycles reachable from the source s. A shortest-paths tree rooted at s is a directed subgraph G'=(V'E'), where V' is in V and E' is in E, such that:
 - V' is the set of vertices reachable from s in G,
 - G' forms a rooted tree with root s, and
 - for all v in V'. the unique simple path from s to v in G' is a shortest path from s to v in G.

Relaxation (technique)
For each vertex v in V, we maintain an attribute v.d, which is an upper bound on the weight of a shortest oath from s to v. We call v.d a "shortest-path estimate".
We initialize the shortest-path estimates and predecessors using the following O(V)-time procedure:
INITIALIZE-SINGLE-SOURCE(G,s)
  for each vertex v in G.V
    v.d = ∞
    v.π = nil
  s.d = 0
The process of "relaxing" an edge (u,v) consists of testing whether we can improve the shortest path to v found so far by going through u and, if so, updating v.d and v.π:
RELAX(u, v, w)  // O(1) time
  if v.d > u.d + w(u,v)
    v.d = u.d + w(u,v)
    v.π = u
Relaxation is the only means by which shortest-path estimates and predecessors change (though various algorithms we'll cover differ in how many times they might relax a given egde).

Properties of shortest paths and relaxation (formally proven in section 24.5)
 - Triangle inequality: for an edge (u,v) in E, we have 𝛿(s,v) <= 𝛿(u,v) + w(u,v)
 - Upper-bound property: We always have v.d >= 𝛿(s,v) for all vertices v in V, and once v.d achieves the value 𝛿(s,v), it never changes.
 - No-path property: If there is no path from s to v, then we always have v.d = 𝛿(s,v) = ∞.
 - Convergence property: If s -> u -> v is a shortest path in G for some u,v in V, and if u.d = 𝛿(s,u) at any time prior to relaxing edge (u,v), then v.d = 𝛿(s,v) at all times afterward.
 - Path-relaxation property: If p=<v0,v1,...,vk> is a shortest path from s=v0 to vk, and we relax the edges of p in the order (v0,v1),(v1,v2),...,(vk-1,vk), then vk.d=𝛿(s,vk). This property holds regardless of any other relaxation steps that occur, even if they are intermixed with relaxations of the edges of p.
 - Predecessor-subgraph property: Once v.d = 𝛿(s,v) for all v in V, the predecessor graph is a shortest-paths tree rooted at s.

-=- The Bellman-Ford algorithm
Solves the SSSP problem for a weighted, directed graph. Edge weights may be negative.
Returns a boolean indicating whether any negative weight cycles are reachable from s. Returns true if no such cycles, the algo produces the shortest paths and their weights.
Runtime: O(EV)
See example in code in this repo. 

-=- Single-source shortest paths in directed acyclic graphs (dags)
By relaxing the edges of a weighted dag according to a topological sort of its vertices, we can compute shortest oaths from a single source in O(V+E).
Sorting imposes a linear ordering of its vertices. If the dag contains a path from vertex u to v, then u precedes in the topological sort.
That means we need to make just 1 pass over the dag's vertices in topological order, and at each vertex, relax each edge that leaves the vertex.
See example in code in this repo of the DAG-SHORTEST-PATHS algo. 

Application:
One application of this algo arises in deterining creitical paths in PERT (program evaluation and review technique) chart anaylists.
Edges represent jobs, and edge weights preresent time to complete them. If edge (u,v) enters vertex v and edge (v,x) leaves v, job (u,v) must be performed before job (v,x).
A path through the dag represents a sequence of jobs that must be performed in a perticular order. A "citical path" is a longest oath through the dag, corresponding to the longest sequence of jobs. Thus, the weight of a critical path provides a lower bound on the time to perform all the jobs.
We can find a critical path by either:
 - negating edge weights and running DAG-SHORTEST-PATHS, or
 - running DAG-SHORTEST-PATHS with ∞ swapped with -∞ in INITIALIZE-SINGLE-SOURCE, and ">" swapped with "<" in the RELAX procedure.

-=- Dijkstra's Algorithm
