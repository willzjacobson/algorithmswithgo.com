Representations of graphs:
G = (V,E), where V is the set of vertices and E is the set of edges
1) adjacency list
  - This is the most common choice.
  - Provides a compact way to represent sparse graphs - those for which E is much less than V^2.
  - Consists of an array "Adj" of lists, one for each vertex in V. Each list u contains all the vertices v such that there is an edge (u,v) in E. That is, Adj[u] consists of all the vertices adjacent to u in G (or pointers to those vertices).
  - For a directed graph, the sum of the lengths of all the adjacency lists is E. For undirected graphs it is 2E (since edges can be said to go both ways). 
  - Requires O(V+E) memory for both directed and undirected graphs. 
  - Weighted graphs can be handled by storing the weight w(u,v) of the edge (u,v) with vertex v in u's adjacency list.
  - potential disadvantage: no quicker way to search for a particular edge (u,v) than to search for v in the adjacency list Adj[u]. An adjacency matrix (see below) remedies this at the cost of more memory.
2) adjacency matrix
  - Useful when the graph is dense - that is, E is close to V^2
  - Also useful when we need to tell quickly if there is an edge connecting 2 given vertices (for example, 2 of the all-pairs-shortest-paths algorithms assume their input graphs are represented by adjacency matrices).
  - We assume all the vertices are numbered. Then, the matrix representation of a graph consist of a V*V matrix A. An entry Aij is 1 if an edge exists between i and j.
  - Requires O(V^2) memory.
  - For an undirected graph, the adjacency matrix is its own transpose, since (u,v) and (v,u) represent the same edge. For this reason, we sometimes only store the entries on and above the diagonal to save memory.
  - To represent a weighted graph, can just store the weight value rather than 1.
  - Though more memory is required for an adjacency matrix than in an adjacency list, they are simpler, and require only 1 bit per entry (unless you're storing the weight rather than just yes/no).

-=- Breadth-first search
The archetype for many important graph algorithms. Prim's minimum-spanning-tree algo and Dijkstra's single-source shortest-path algo use similar ideas.
Starting with a source vertex s, breadth-first search discovers all vertices at distance k from s before discovering vertices at distance k+1.
BFS uses node colors and a Queue as a tool to keep track of traversal. 
  - At the start, all nodes are white except the "source" vertex, which is gray.
  - For each vertex connected to s, we:
    - turn it gray because it is now "discovered". 
    - Set its "parent" attribute to vertex s.
    - Set it's "distance" attribute to be 1 + that of its parent's (its distance from s)
    - Enqueue it so it is later processed just like s is being processed now. 
  - We then turn s from gray to black, having fully processed it.
  - While the queue has length, we continually pop off vertices and process them as above.
By adding the "parent" attribute to each vertex connected to s, the BFS algorithm produces a subgraph that we called a "breadth-first tree" (it is in fact a tree, since |Eπ| = |Vπ| - 1), which is itself a graph denoted by Gπ = (Vπ, Eπ) that contains all vertices reachable from s. 
The breadth-first tree records a shortest path between each vertex and s.
Runtime:
Each vertex is handled by the queue once, and when it is, we handle each one of its edges. Thus, the runtime of BFS is O(V+E), That is, it's linear with respect to the adjacency list-representation. 

-=- Depth-first search
DFS is often used as a subroutine in another algorithm (as we shall see later).
Works on directed and undirected graphs.
Search "deeper" in the graph whenever possible:
  - Explores edges out of the most recently discovered vertex v that still has unexplored edges leaving it. 
  - Whenever a vertex v is discovered, we set v.P to the vertex from which it is discovered (as in BFS).
  - Once all of v's edges are explored, the seaerch "backtracks" to explore edges from the vertex from which v discovered. 
  - This continues until we have reached all vertices reachable from the original source vertex. 
  - If any undiscovered vertices remain, we repeat the process for them until every vertex has been discovered.
  - Like BFS, we use the colors to keep track of where we've been, so each node only gets added to 1 tree.
  - Each vertex also gets 2 timestamps (time is kept globally by an incrementing integer): one marking the time when v is discovered (grayed), and once when it is finished (blackened). Timestamps are between 1 and 2V, since each vertex is stamped twice.
Unlike BFS, the predecessor graph generated by DFS may consist of several trees, since the search may repeat from multiple sources. We define this predecessor graph as:
  Gπ = (V, Eπ), where 
  Gπ is a "depth-first forest" comprised of several "depth-first trees", and
  the edges in Eπ are the tree edges.
The exact graph resulting from running DFS will vary based on the order of the nodes processed, and the order of edges recorded in the adjacency-list. This generally does not matter.
Runtime:
We call the helper DFSVisit once on each vertex (O(V)), and the function loops through all the vertices adjacent to that vertex, only calling itself again for vertices that are still white (O(E)). Thus, runtime for DFS, like BFS, is O(V+E).

Properties of DFS
- The structure of the trees in the resulting forest exactly mirrors the structure of the recursive calls of DFSVisit.
- Theorem: The processing of nodes form a parenthetical structure. That is, for any 2 vertices u and v, exactly 1 of the following conditions is true:
  1. The intervals [u.start, u.end] and [v.start, v.end] are disjoint, and neither u nor v is a descendant of the other in the depth-first forest.
  2. [u.start, u.end] is entirely contained within [v.start, v.end], and u is a descendant of v in a depth-first tree.
  3. [v.start, v.end] is entirely contained within [u.start, u.end], and v is a descendant of u in a depth-first tree.
- Theorem: vertex v is a proper descendant of vertex u in the depth-first forest for a (directed or undirected) graph if and only if u.start < v.start < v.end < u.end.
- "White path theorem": In a depth-first forest of a (directed or undirected) graph, vertex v is a descendent of vertex u if and only if at the time u.start, there is a path connecting u to v consisting of only white vertices.

Classification of edges
DFS can be used to classify edges of an input graph, and the type of edge can provide info about the graph. For example, we can tell that a directed graph is acyclic if and only if a depth-first search yields no "back" edges (since that would mean the descendant has a path to the ancestor that is different from the path DFS has already taken from the ancestor to the descendant).
4 types of edges:
  1) "Tree edges" are edges in the depth-first forest Gπ. Edge (u,v) is a tree edge if v was first discovered by exploring edge (u,v). (going down)
  2) "Back edges" are those edges (u,v) connecting u to an ancestor v in a depth-first tree. This includes self-loops, which may occur in directed graphs. (ran out of white vertices, going back up)
  3) "Forward edges" are those nontree edges (u,v) connecting a vertex u to a descendent v in a depth-first tree. (not a tree edge, since the descendent is no longer white)
  4) "Cross edges" are all other edges. They can go between vertices in the same DF tree, as long as one vertex is not an ancestor of the other, or they can go between vertices in different DF trees.
During DFS, when we explore the edge (u,v), the color of v tells us something about the edge:
 - White indicates a tree edge
 - Gray indicates a back edge
 - Black indicates a forward or cross edge. The edge (u,v) is a forward edge if u.start < v.start, and a cross edge if u.start > v.start.
An undirected graph has ambiguity as to the type of each edge, since each edge occurs twice. Don't worry about it - whichever you encounter first.
Theorem: In a DFS of an undirected graph, every edge is either a tree edge or a back edge.

Topological sorting of a directed ascyclic graph (dag) using DFS
Topological sorting is a linear ordering of vertices such that for every directed edge (u,v), u comes before v in the ordering. 
Topological sorting of a graph is not possible if the graph is not a dag.

-=- Strongly Connected Components
A classic application of DFS is decomposing a directed graph into its strongly connected components (SCCs). It is a common first step in many graph algos, and we can do it using 2 depth-first searches.
A strongly connected component of a graph is a maximal set of vertices such that every pair of vertices are reachable from each other.
Our algo for finding the SCCs of a graph G uses the transpose of G, which we define as GT=(V, ET), where ET consists of the edges of G with their directions reversed. The time to compute GT from the adjacency-list representation of G is O(V+E).
Note that G and GT have the same SCCs: vertices u and v are reachable from each other in G if and only if they are reachable from each other in GT. 
The following linear time O(V+E) algorithm computes the SCCs of a directed graph G=(V,E) using 2 depth-first searches, one on G and one on GT:
STRONGLY-CONNECTED-COMPONENTS(G)
  1. call DFS(G) to compute finishing times u.f for each vertex u
  2. compute GT
  3. call DFS(GT), but in the main loop of DFS, consider the vertices in order of decreasing u.f as computed in line 1 (that is, in topologically sorted order)
  4. output the vertices of each tree in the depth-first forest formed in step 3 as a separate SCC
The key property is that the component graph is a dag (if it was not acyclic, then we would not have distinct SCCs).

Why it works:
- Theorem: Let C and C' be distinct SSCs in a directed graph G=(V,E). Suppose there is an edge (u,v) in E, where u is in C and v is in C'. Then, f(C) > f(C'). That is, the max finishing time in the tree C is greater than that of C'. This is true regardless of whether any vertex in C is discovered by DFS before any vertex in C', or vice versa.
- Corrollary: The above theorem tells us that each edge in GT that goes between distinct SSCs, goes from a component with an earlier finishing time in the DFS to a component with later finishing time. That is, if there in an edge (u,v) in ET, where u in C and v in C', then f(C) < f(C').
- The above corrollary tells us why the SCC algo works: 
  - When we perform the 2nd DFS (on GT) in step 3, we start with some vertex in the SCC whose finishing time in the 1st DFS is greater than all the other SCCs. The search starts from some vertex in C, and visits all other vertices in C. The above corrollary tells us that if C contains no vertices that point to other SCCs. Thus, the resulting depth-first tree rooted at x contains the vertices in C and no others. 
  - The next white node y that the DFS in step 3 finds is a member of the SCC C' with the second greatest finish time of the SCCs. Vertices in C' only contain edges pointing to other vertices in C', and to C (but all vertices in C are no longer white). Thus, the resulting depth-first tree rooted at y contains the vertices in C' and no others. And so on for each SCC. 
  - Therefore, each highest level VISIT call in the 2nd DFS generates a depth-first tree representing an SCC.


-=-=- Minimum Spanning Trees
Example of the problem: We have n electric pins in a circuit that we want to connect. We can use an arrangement of n-1 wires, each connecting 2 pins. We want to find the configuration that uses the least amount of wire possible. For each edge (u,v) we have a weight, which is the cost of wire to connect pins u and v. We call the result a minimum spanning tree (it is a tree, since it's acyclic and connects all the vertices). 
We can solve the minimum spanning tree problem using Krushal's algo and Prim's algo (both greedy algorithms). Each are O(E lg V) using a binary heap. By using Fibonacci heaps, we can make Prim's algo run in O(E + V lg V), which improves performance if V is much smaller than E.

Useful definitions:
 - A "cut" (S, V - S) of an undirected graph G=(V,E) is a partition of V. 
 - We say an edge (u,v) "crosses" the cut if one of its endpoints is in S and the other is in V-S. 
 - We say that a cut "respects" a set A of edges if no edge in A crosses the cut. 
 - An edge is a "light edge" crossing a cut if its weight is the minimum of any edge crossing the cut. There can be >1 light edges in the case of ties.

  Theorem: Let:
  - G = (V, E) be a connected, undirected graph with a real valued weight function w defined on E,
  - A be a subset of E that is included in some miminim spanning tree for G, 
  - (S,V-S) be any cut of G that respects A, and
  - (u,v) be a light edge crossing (S,V-S).
Then edge (u,v) is safe for A. That is, if (u,v) is added to A, A will still be a subset of E that is included in a mimimum spanning tree for G.
  Corrollary: Let:
  - G = (V, E) be a connected, undirected graph with a real valued weight function w defined on E,
  - A be a subset of E that is included in some miminim spanning tree for G, 
  - C = (Vc, Ec) be a connected component (tree) in the forest Ga = (V, A).
If (u,v) is a light edge connecting C to some other component in Ga, then (u,v) is safe for A.
Proof: the cut (Vc, V-Vc) respects A, and (u,v) is a light edge for this cut. Therefore, (u,v) is safe for A.

-=- Kruskal's Algorithm
Finds a safe edge to add to the growing forest by finding, of all the edges that connect any 2 trees in the forest, an edge (u,v) of least weight. See programmed example.
Runtime: O(E lg V)  (the logic used to figure this out is kinda complex, see p.633)

-=- Prim's Algorithm
Has the property that the edges in the set A always form a single tree, which starts with an arbitrary root r, and grows until the tree spans all the vertices V. Each step adds to the tree A a light edge that connects A to an isolated vertex.
Runtime: O(E lg V) if using a binary heap, O(E + lg v) if using Fibonacci Heap (analysis on p. 636).


-=-=- Single-Source Shortest Paths (SSSP)
In a "shortest-paths" problem, we are given a weighted, directed graph G=(V,E), with weight function w : E -> R mapping edges to real-valued weights. The weight w(p) of path p = <v0,v1,...vk> is the sum of the weights of its constituent edges.
We denote the shortest path weight from vertex u to v with the symbol 𝛿(u,v). If there is no path from u to v, we say 𝛿(u,v)=∞. 
A shortest path from vertex u to vertex v is then defined as any path p with weight w(p) = 𝛿(u,v).
One great application is to model a roadmap, and determine the shortest distance between 2 intersections. However, the weights can also be used to represent other metrics than distance, such as time, cost, penalties, loss, or any other quantity that accumulates linearly along a path that we want to minimize.

Variants:
 - SSSP problem: Find a shortest path to a given destination vertex t from each vertex v.
 - Single-pair shortest-path problem: find a shortest path from u to v for given vertices u and v. If we solve the single-source problem, we solve this one (and the 2 have the same runtime).
 - All-pairs shortest-paths problem: Find a shortest path from u to v for every pair of vertices u and v.

Substructure
Shortest-paths algos typically rely on the property that a shortest path between 2 vertices contains other shortest paths within it.
 - Lemma: Given a weighted, directed graph G=(V,E) with weight function w: E -> R, let p=<v0, v1,...,vk> be a shortest path from vertex v0 to vk, and for any i and j such that 0<=i<=j<=k, let pij=<pi,pi+1,...,pj> be the subpath of p from vertex vi to vertex vj. Then, pij is a shortest path from vi to vj.

Negative weights
Some instances of SSSP problems may include edges with negative weights. So long as no negative weight cycles are reachable from source s, the shortest-path weight 𝛿(s,v) is well defined.

Cycles
We will have no negative weight cycles, positive weight cycles, or 0-weight cycles in our shortest paths (they are simple paths). Thus, any shortest path will traverse at most V distinct vertices, and V-1 edges.

Representation
We represent shortest paths similarly to how we did in BFS: with each vertex holding a pointer to the previous vertex along the path. 
PRINT-PATH(G,s,v) then prints out the path from source vertex s to vertex v. 
Like with BFS, we'll be concerned with the "predecessor subgraph" Gπ=(Vπ, Eπ), where Vπ is the set of vertices of G with non-nil predecessors, plus the source s.
The directed edge set Eπ is the set of edges induced by the π values for vertices in Vπ.
We will later prove that at termination, Gπ is a "shortest-paths tree": a rooted tree containing a shortest path from the source s to every vertex reachable from s.
Specifically, let G=(V,E) be a weighted, directed graph with no negative weight cycles reachable from the source s. A shortest-paths tree rooted at s is a directed subgraph G'=(V'E'), where V' is in V and E' is in E, such that:
 - V' is the set of vertices reachable from s in G,
 - G' forms a rooted tree with root s, and
 - for all v in V', the unique simple path from s to v in G' is a shortest path from s to v in G.

Relaxation (technique)
For each vertex v in V, we maintain an attribute v.d, which is an upper bound on the weight of a shortest path from s to v. We call v.d a "shortest-path estimate".
We initialize the shortest-path estimates and predecessors using the following O(V)-time procedure:
INITIALIZE-SINGLE-SOURCE(G,s)
  for each vertex v in G.V
    v.d = ∞
    v.π = nil
  s.d = 0
The process of "relaxing" an edge (u,v) consists of testing whether we can improve the shortest path to v found so far by going through u and, if so, updating v.d and v.π:
RELAX(u, v, w)  // O(1) time
  if v.d > u.d + w(u,v)
    v.d = u.d + w(u,v)
    v.π = u
Relaxation is the only means by which shortest-path estimates and predecessors change (though various algorithms we'll cover differ in how many times they might relax a given egde).

Properties of shortest paths and relaxation (formally proven in section 24.5)
 - Triangle inequality: for an edge (u,v) in E, we have 𝛿(s,v) <= 𝛿(u,v) + w(u,v)
 - Upper-bound property: We always have v.d >= 𝛿(s,v) for all vertices v in V, and once v.d achieves the value 𝛿(s,v), it never changes.
 - No-path property: If there is no path from s to v, then we always have v.d = 𝛿(s,v) = ∞.
 - Convergence property: If s -> u -> v is a shortest path in G for some u,v in V, and if u.d = 𝛿(s,u) at any time prior to relaxing edge (u,v), then v.d = 𝛿(s,v) at all times afterward.
 - Path-relaxation property: If p=<v0,v1,...,vk> is a shortest path from s=v0 to vk, and we relax the edges of p in the order (v0,v1),(v1,v2),...,(vk-1,vk), then vk.d=𝛿(s,vk). This property holds regardless of any other relaxation steps that occur, even if they are intermixed with relaxations of the edges of p.
 - Predecessor-subgraph property: Once v.d = 𝛿(s,v) for all v in V, the predecessor graph is a shortest-paths tree rooted at s.

-=- The Bellman-Ford algorithm
Solves the SSSP problem for a weighted, directed graph. Edge weights may be negative.
Returns a boolean indicating whether any negative weight cycles are reachable from s. Returns true if no such cycles, and produces the shortest paths and their weights.
Runtime: O(EV)
See example in code in this repo. 

-=- Single-source shortest paths in directed acyclic graphs (dags)
By relaxing the edges of a weighted dag according to a topological sort of its vertices, we can compute shortest paths from a single source in O(V+E).
Sorting imposes a linear ordering of its vertices. If the dag contains a path from vertex u to v, then u precedes in the topological sort.
That means we need to make just 1 pass over the dag's vertices in topological order, and at each vertex, relax each edge that leaves the vertex.
See example in code in this repo of the DAG-SHORTEST-PATHS algo. 

Application:
One application of this algo arises in determining critical paths in PERT (program evaluation and review technique) chart anaylists.
Edges represent jobs, and edge weights preresent time to complete them. If edge (u,v) enters vertex v and edge (v,x) leaves v, job (u,v) must be performed before job (v,x).
A path through the dag represents a sequence of jobs that must be performed in a particular order. A "citical path" is a longest path through the dag, corresponding to the longest sequence of jobs. Thus, the weight of a critical path provides a lower bound on the time to perform all the jobs.
We can find a critical path by either:
 - negating edge weights and running DAG-SHORTEST-PATHS, or
 - running DAG-SHORTEST-PATHS with ∞ swapped with -∞ in INITIALIZE-SINGLE-SOURCE, and ">" swapped with "<" in the RELAX procedure.

-=- Dijkstra's Algorithm
Solves the single-source shortest-paths problem on a weighted graph when all edges have nonegative weights. Why bother when we already have Bellman-Ford algo? Lower runtime.
The algorithm:
  - Maintains a set S of vertices whose final shortest-path weights from source s have already been determined. 
  - Repeatedly selects the vertex u from V-S with the minimum shortest path estimate, adds u to S, and relaxes all edges leaving u.
  - Uses a min priority queue of vertices, keyed by their d (distance from source) values.
The resulting predecessor subgraph Gπ is a shortest-paths tree rooted at source vertex s.
Dijkstra's algo resembles BFS in that it assigns vertices their distances the first time it sees them. It also resembles Prim's algo in that both use a min queue to find the nearest "white" vertex outside a given set.
See example in code in this repo of DIJKSTRA's algo.
Runtime:
 - Initializing the min queue is O(V) (that's not in a loop, so winds up not mattering)
 - The while loop runs once for each index.
   - we extract the min from the queue O(1)
   - we loop through each edge an index has (since we're in the loop that brings us to O(V+E))
     - we call relax, which implicitly has a Queue.DecreaseKey call, which is on its own is O(lg v)
Thus, using a binary heap, we're at O((lg V)*(V+E)). That becomes O(E lg V) if all vertices are reachable from the source.
Potential improvement: a Fibonacci heap would do ExtractMin in O(lg V), and DecreaseKey in O(1). Thus, if we swap out a the binary heap for a Fibonacci heap, we could acheive O(Vlg v + E).
Historically, development Fibonacci heap was actually motivated by the fact that Dijkstra's algo typically makes many more DecreaseKey calls than ExtractMin calls. So in this case, a Fib heap lets us shift the cost of the queue operations up out of the inner loop. 

-=- All-Pairs Shortest Paths
Here we consider the problem of, given a weighted, directed graph, finding shortest paths between all pairs of vertices in a graph. For example, making a table of distances between all paris of cities for a road atlas. 
In these problems we typically want the output in tabular form: the entry in u's row and v's column should be the weight of a shortest path from u to v.
We can solve these problems by running single-source shortest-paths algos |V| times. 
If all edge weights are nonnegative, we can use Dijkstra's algo:
 - using the linear-array implementation of the min-priority queue, the running time is O(V^3 + VE) = O(V^3).
 - the binary min-heap queue implementation yields a running time of O(V E lg V), which is an improvement if the graph is sparse.
 - If we use a Fibonacci heap for the queue, that yields O(V^2 lg V + VE)
If the graph has negative edges, we use the slower Bellman-Ford algo once for each vertex:
 - the resulting running time is O(V^2 E), which on a dense graph is O(V^4).
 - We will show how to do better than that below.

These methods typically represent the graph with an adjacency-matrix. We number vertices 0 - |v|-1, so that the input is an nxn matrix W representing the edge weights of an n-vertex directed graph G=(V,E). That is, W=(wij), where wij=0 if i=j, the weight of the directed edge (i,j), or ∞ if there is no edge (i,j) in G. Allow negative edge weights, but assume for now no negative cycles exist.
The tabular output of the all-pairs shortest-paths algos is represented in an nxn matrix D=(dij), where entry dij contains the weight of a shortest path from vertex i to vertex j. 

However, we need to compute not only the shortest-paths weights, but also a predecessor matrix. π, where πij is nil if i=j or there is no path from i to j, or πij is the predecessor of j on a shortest path from i to j.
Just as the predecessor subgraph referred to above is a shortest-paths tree for a given source vertex, the subgraph induced by the ith row of the π matrix should be a shortest-paths tree with root i. 
For each vertex i in V, we define the predecessor subgraph of G for i as Gπ,i = (Vπ,i, Eπ,i), where:
 - Vπ,i is a set of vertices j for which πij != nil.
 - Eπ,i is a set of edges (πij,j), where j is in Vπ,i, not including i.
The procedure PrintAllPairsShortestPaths prints a shortest path from vertex i to vertex j (code in this repo).

conventions:
 - input graph G=(V,E) has n vertices, so |V| = n.
 - denote matrices by uppercase letters (such as W, L, or D) and their individual elements by subscripted lowercase letters (wij, lij, dij).
 - some matrices will have parenthesized superscripts, as in L(m) = (l(m)ij), or D(m) = (d(m)ij) to indicate iterates.
 - assume in the examples in the notes that the number of rows in an nxn matrix a is stored in the attribute A.rows.

Shortest Paths and matrix multiplication
Here we present a dynamic-programming algo for the all-pairs shortest-paths problem in a directed graph G=(V,E).
Each loop will invoke an operation similar to matrix multiplication. We'll start with a O(V^4) implementation, then get it down to O(V^3 lg V).

O(V^4) implementation:
We know the shortest paths problems already have optimal substructure for dynamic-programming, since all subpaths of a shortest path are themselves shortest paths.
Our initial solution starts with the observation that W is the shortest-paths matrix when considering paths that have m=1 edge.
It then performs n-2 loops, extending the shortest paths computed so far by 1 edge each time through loop using a helper function EXTEND-SHORTEST-PATHS(L,W).
EXTEND-SHORTEST-PATHS(L,W) takes the previous shortest paths matrix L(m), which contains all shortest paths of maximum length m, and uses W to compute L(m+1).
We do this n-1 times, since all shortest paths will have at most n-1 edges if G contains no negative weight cycles.
EXTEND-SHORTEST-PATHS builds the next L(m) from W and L(m-1) using this key operation:
  l(m)ij = min(l(m-1)ij, l(m-1)ik + wkj), where for k we try all possible predecessors of vertex j.
The EXTEND-SHORTEST-PATHS operation resembles matric multiplaction in its triple nested loop structure, and runs in O(V^3). Looping through it |V| times this takes O(V^4) time.

Improving the O(V^4) Implementation
We don't actually want all the L(m) matrices; we only really care about L(n-1).
Due to the similarity between EXTEND-SHORTEST-PATHS and matrix multiplaction, we can sort of summarize our algo thus far like this:
  L(1) = L(0) * W = W
  L(2) = L(1) * W = W2
  L(3) = L(2) * W = W3
  ...
  L(n-1) = L(n-2) * W = Wn-1
Just as traditional matrix multiplication is associative, so is the matrix operation defined by EXTEND-SHORTEST-PATHS. We cat therefore compute L(n-1) with only lg(n-1) matrix products via the sequence:
  L(1) = W
  L(2) = W * W
  L(4) = W2 * W2
  L(8) = W4 * W4
  ...
etc. up until m >= n-1. This technique is called "repeated sqauring". (see example implementation in this repo).
This gives us a running time of O(lg v V^3).

-=- Floyd-Warshall algorithm
Like the algo laid out above, we assume a directed graph G-(V,E) that can have negative weight edges, but no negative weight cycles.
The FW algo also uses dynamic-programming, but uses a different structure of a shortest path and runs in O(V^3) time.

The FW algo considers the "intermediate" vertices of a shortest path p=<v1, v2,...,vl>; that is, all vertexes other than v1 and vl.
We rely on the following observation: 
 - asssuming the vertices of graph G are V={1,2,...,n}, let us consider a subset {1,2,...,k} of vertices for some k.
 - For any pair of vertices i,j in V, consuder all paths from i to j whose intermediate vertices are all drawn from {1,2,...,k}, and let p be a minimum-weight path from among them.
 - The FW algo exploits a relationship between path p and shortest paths from i to j with all intermediate vertices in the set {1,2,...,k-1}. The relationship depends on whether or not k is an intermdiate vertex of path p:
   - if k is not an intermediate vertex of path p, all intermediate vertices of path p are in the set {1,2,...,k-1}. Thus, a shortest path from i to j with all intermdiate vertices in {1,2,...,k-1} is also the shortest path from i to j with all intermdiate vertices in {1,2,...,k}.
   - if k is an intermediate vertex of path p, we decompose p into p1 and p2, where p1 is a shortest path from i to k with intermediate vertices in the set {1,2,...,k-1} and p2 is a shortest path from k to j with intermediate vertices in the set {1,2,...,k-1}.

We now create a recursive solution from the above observations.
lLt d(k)ij be the weight of a shortest path from vertex i to vertex j for which all intermdiate vertices are in the set {1,2,...,k}.
When k=0, the path has no intermdiate edges. Hence, d(o)ij = wij.
When k>0, we define d(k)ij recursively as min(d(k-1)ij, d(k-1)ik + wkj).
Since for any path, all intermdiate vertices are in the set {1,2,...,n}, the matrix D(n)=(d(n)ij) gives the final answer: d(n)ij=𝛿(i,j) for all i,j in V.

Computing shortest-path weights bottom-up
Based on the above recursive equation, we can copute the values d(k)ij in order of increasing values of k.
The input of the procedure FLOYD-WARSHALL(W) is an nxn matrix W. The output is the matrix D(n) of shortest path weights.
Due to thrice nested loop, running time of the FW algo is O(V^3). The constant cost is low, often making FW a good choice even for small data sets.
(see example implementation in this repo).

Reconstructing shortest paths
In our FW implementation, we also compute a sequence of matrices π(0),π(1),...,π(n), where π=π(n). 
We define π(k)ij as the predecessor of vertex j on a shortest path from vertex i with all intermediate vertices in the set {1,2,...,k}.
When k=0:
  - if i=j or there is no path from i to j with all intermediate vertices in the set {1,2,...,k}, π(k)ij = nil.
  - if i != j and there is a path from i to j with all intermediate vertices in the set {1,2,...,k}, π(k)ij = i.
When k>1, π(k)ij =
  - π(k-1)ij if d(k-1)ij <= d(k-1)ik + d(k-1)kj
    no change since adding vertex k to the set of allowed intermdiate vertices
  - π(k-1)kj if d(k-1)ij > d(k-1)ik + d(k-1)kj
    shortest path has change since adding vertex k to the set of allowed intermdiate vertices. The predecessor to j is thus the same as the predecessor to j on a shortest path from k->j. We know this value already exists in π, since k is not an intermediate vertex on the path k->j. Clever!

-=- Transitive closure of a directed graph
It may be useful to know whether a path exists between any 2 vertices on a directed graph G.
We define the "transitive closure" of G as G*=(V,E*), where E* is the set of all edges (i,j) where a path exists from vertex it to vertex j in G.
One way to compute G* in O(V^3) time is to assign a weight of 1 to each edge and run the FW algo. If there is a path from i->j, we get dij<n. Otherwise, dij=∞.

However, there exists another O(V^3) method that is often faster in practice, since it uses booleans and logical operators rather than numbers and arithmetic.
For i,j,k=1,2,...,n, we define t(j)ij to be true if there exists a path from i->j with all intermediate vertices in the set {1,2,...,k}, and false otherwise.
A recursive definition of t(k)ij is:
  for k=0, t(0)ij=
    0 if i != j and no edge exists between i and j
    i if i == j or an edge exists between i and j
  for k>0, t(k)ij = t(k-1)ij || t(k-1)ik && t(k-1)kj
    that is, t(k)ij will be true if either t(k-1)ij is true, or if t(k-1)ik=true and t(k-1)kj=true (a path exists between i and k, and between k and j).

-=- Johnson's algorithm for sparse graphs
Johnson's (J) algorith finds shortest paths between all pairs in O(V^2 lg V + VE) time. It is asymptotically faster than either repeated squaring or the FW algo for sparse graphs (where E < V^2).
The algo either returns a matrix of shortest-path weights for all pairs of vertices or reports that the input graph contains a negative-weight cycle.
J uses both Dijkstra's algo and the Bellman-Ford algo.

J uses "reweighting":
  - If all edge weights w in a graph G=(V,E) are nonnegative, we can find shortest paths between all pairs of vertcies using Dijkstra once from each vertex. With the Fibonacci heap min-priority queue, running time of this solution is O(V^2 lg V +VE). 
  - If G has negative weight edges but no negative-weight cycles, we compute a new set of nonnegative edge weights that allows us to use the same method. New set of edge weights w' must satsify the following 2 properties:
    - shortest paths between all pairs of vertices calculated using the weight function w' must be the same as those calculated using w.
    - for all edges (u,v), the new weight w'(u,v) is nonnegative.
  - We can preprocess G to determine the new weight function w' in O(VE) time.

Reweighting preserves shortest paths
Given a weighted, directed graph G=(V,E) with weight function w : E->R, let h : V -> R be any function mapping vertices to real numbers.
Then, for each edge (u,v) in E, we define:
  w'(u,v) = w(u,v) + h(u) - h(v).
Lemma: Let p=<v0,v1,...,vk> be any path from vertex v0 to vk. Then:
  1) p is a shortest path from v0->vk with weight function w if and only if it is a shortest path with weight function w'. That is w(p)=𝛿(v0,vk) if and only if w'(p)=𝛿'(v0,vk). 
    Proof: This is true because
      sum i->k( w'(vi-1,vi) ) = sum i->k( w(vi-1,vi) + h(vi-1) - h(vi) ) = sum i->k( w(vi-1,vi) ) + h(v0) - h(vk) = w(p) + h(v0) - h(vk).
    h(v0) and h(vk) do not depend on the path, so if 1 path from v0->vk is shorter than another using w, the same is true using w'.
  2) G has a negative-weight cycle using weight function w if and only if G has a negative-weight cucle using weight function w'. 
    Proof: Consider any cycle c=<v0,v1,...,vk>, where v0=vk.
    We have w'(c) = w(c) + h(v0) - h(vk) = w(c). 
    Thus, c has negative weight using w if and only if it has negative weight using w'.

Producing nonnegative weights by reweighting
Now we must ensure w'(u,v) >= 0 for all edges (u,v) in E.
Given a weighted, directed graph G=(V,E), we make a new graph G'=(V',E'), where:
  - V' = V u s (s is a new vertex)
  - E' = E u (s,v) for all v in V. 
  - w(s,v)=0 for all v in V. 
  We know that no shortest paths go through s since no edges point to s. Also, G' has no negative cycles if and only if G has no negative cycles.
Now, suppose G and G' have no negative-weight cycles. Let us define
  h(v) = 𝛿(s,v) for all v in V. By the "triangle inequality" (from 24.10: 𝛿(s,v) <= 𝛿(s,u) + w(u,v)), we have:
  h(v) <= h(u) + w(u,v) for all edges (u,v) in E'. 
Moving things around gives us:
  0 <= w(u,v) + h(u) - h(v). 
Since w'(u,v) = w(u,v) + h(u) - h(v), we have:
  w'(u,v) >= 0.

The Algo
Johnson's Algo assumes the edges are stored in adjacency-lists. 
It returns the usual |V| X |V| matrix D=dij, where dij = 𝛿(i,j), or it reports that the input graph contains a negative weight cycle.
As is typical in an all-pairs shortest-paths algo, we assume the vertices are numbered from 1 to |V|.
running time:
  - Using a Fibonacci heap for the min-queue in Dijksctra's algo yields O(V^2 lg V + VE) for J's algo.
  - A binary heap yields O(VE lg V), which is still better than the Floyd-Warshall algo if the graph is sparse (such as for a roadmap of cities).
