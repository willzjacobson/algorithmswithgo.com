
Definitions
 - In place: A sorting algorithm sorts "in place" if only a constant number of elements of the input array are ever stored outside the array.
 - Comparison sorts: algorithms that determine the sorted order of an input array by comparing elements (quick, merge, bubble, insertion, heap).
 - Linear sorting: algorithms that treat the problem of sorting as a single large operation.
 - Divide and Conquer: algorithms that partition the data to be sorted into smaller sets that can be independently sorted.

Performance Considerations
 - Comparisons and Swaps both have a cost
 - Reducing either or both can improve performance
 - The cost of both operations depends on many factors.

-=- Comparison Sorting Methods
They work by comparing one value in a collection against another
The best these can do under worst case conditions is nlogn

Insertion Sort
O(n^2) worst case
O(n^2) average case
O(n) best case
Space required: O(n) (in place; linear space complexity)
Loops are tight, so fast-paced for small sizes and nearly sorted data sets
Summary:
 - Sorts each item in the array as they are encountered 
 - As the current item works from left to right
  - Everything left of the item is known to be sorted
  - Everything to the right is unsorted
 - The current item is “inserted” into place within the sorted section

Merge Sort (divide and conquer)
O(nlong) worst case
O(nlong) average case
O(nlong) best case
Can be, but is not generally performed in place. Hence, space required sometimes > O(n)
Summary:
 - The array is split in half recursively
 - When the array is in groups of 1, it is reconstructed in sort order
 - Each reconstructed array is merged with the other half

Bubble Sort
O(n^2) worst case
O(n^2) average case
O(n) best case (if already sorted)
Space required: O(n) (in place)
Performs well on small sample sizes that are already quite well sorted
Summary:
On Each Pass
 - Compare each array item to it’s right neighbor
 - If the right neighbor is smaller then swap right and left
 - Repeat for the remaining array items

Selection Sort
O(n^2) worst case
O(n^2) average case
O(n^2) best case
Space required: O(n) (in place)
Summary:
 - Enumerate the array from the first unsorted item to the end
 - Identify the smallest item
 - Swap the smallest item with the first unsorted item

Quick Sort (divide and conquer)
O(n^2) worst case 
  not good for inverse sorted, or sorted data sets, as these result in unbalanced partitioning
  since this is common in engineering scenarios, we randomize the placement of the pivot in the array to partition
O(nlogn) average case
  much closer to the best case than the worst case,
  as even a 99-1 split runs asymtotically in nlogn time. Only a 100-0 split runs in n^2 time
O(nlogn) best case
  occurs with balanced partitioning, which results in 2 arrays of length <= n/2
Space required: O(n) (in place)
Often the best choice, as the expected time is nlogn and the hidden constant factors are small
Recursive algorthm, so stack space must be considered.
Summary:
 - Pick a pivot value and partition the array
 - Put all values before the pivot to the left and above to the right
 - The pivot point is now sorted – everything right is larger, everything left is
smaller.
 - Perform pivot and partition algorithm on the left and right partitions
 - Repeat until sorted

Heap Sort
O(nlogn) worst case
O(n) space complexity
Heapsort is great, but a good implementation of quicksort usually beats it in practice. 
However, the underlying data structure (heap) is very useful. For example, for implementing priority queues.

-=- Linear Sorting Methods
Work by means other than just comparing values
Thus, the lower bound of nlogn that plagues comparison sorting methods does not apply. 

Counting Sort
Use case: have n integers to sort, where the integers rangs from 0 to k
O(k+n) worst case
O(k+n) average case 
Performs the best where k is low compared to n
Counting sort is "stable", meaning that the order of elements with the same value is not changed. This is important in cases where satelite data is carried along with the value we're sorting on. It is also useful because it allows counting sort to be used as a subroutine in radix sort. 

Radix Sort
Use case: n elements to sort, each item is a d-digit number, where each digit takes on k possible values. Used by old card sorting machines. Useful when we want to sort information that is keyed by multiple fields. For example, dates by 3 keys: year, month, and day.
O(d(k+n)) worst case
O(d(k+n)) average case
Summary:
Use a stable sort to sort on the least significant field. Then do the same on the 2nd least significant field, etc. ... through the most significant field. 
Is radix sort faster than quicksort? Sometimes. But the constants are higher in radix sort, and quick can be faster. And radix sort it is not in place, so quicksort can be better when memory storage is at a premium.
UNABLE TO IMPLEMENT -- RETURN TO THIS

Bucket Sort
Assumes the unput is drawn from a uniform distribution between 0 and 1 (I assume you could normalize data that is of uniform distribution but not between 0 and 1) (this assumption allows it to be fast).
O(n^2) worst case (basically becomes worst case insertion sort)
O(n) average case
Even if the input is not of uniform distribution, bucket sort will still run in linear time if the sum of the quares of the bucket sizes is linear in the total number of elements. 
Summary:
Let b[0..n-1] be a new array of empty linked lists
for i = 1 to n
  insert A[i] into list b[n*A[i]]
for i=0 to n-1
  sort list in b[i] using insertion sort
concatinate the lists b[0]...b[n-1] together in order
