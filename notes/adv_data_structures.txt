B-Trees
Similar balanced search trees designed to work well on dists or other direct-access secondary storage devices. They are similar to red-black trees, but better at minimizing disk I/O operations. Many database systems use B-trees (or variants thereof) to store information.
B-trees differ from red-blakc trees in that each node may have as many as thousands of children (that is, the branching factor can be large). 
B-trees are similar to red-black trees in that each node has height O(lgn). However, since the height can be much less, the base of the log that expresses its height can be much larger.
If an internal B-tree node x contains x.n keys, then it has x.n+1 children. They keys in x serve as dividing points eparating the rangs of keys handled by x into x.n+1 subranges. Each subrange is handled by a child of x. 
Disk operations are much slower than main memory operations. If we have a B-tree stored on disk, a large branching factor reduces the height of the tree, and thus reduces the number of disk operations that must be performed to find and operate on a node in a B-tree. Thus, a B-tree node is usually the size of a wole disk page (the amount of data that can be read/written in a single disk action), and is limited by the page size.
Lets say we have a B-tree stored in memory that has a branching factor of 1000. If we keep the root node stored in main memory (RAM; much faster look-up than disk), we can find any of the 1 billion nodes with at most 2 disk look-ups. 
A B+-tree is a common variant of a B-tree that stores all the satelite info in the leaves and stores only keys and child pointers in the internal nodes, thus maximizing the branching factor of the internal nodes.

B-tree definition
A B-tree is a rooted tree (whose root is T.root) having the following properties:
1) Every node x has teh following attributes:
  a) x.n, the number of keys currency stored in node x
  b) the x.n keys are stored in non-decreasing order
  c) x.leaf, a boolean value that is TRUE if x is a leaf and FALSE if x is an internal node
2) Each internal node x also contains x.n+1 pointers to its children x.c1, x.c2,...,x.cx.n+1. Leaf nodes have no children, so their ci attributes are undefined.
3) The keys x.keyi separate the ranges of keys stored in each subtree: if ki is any key stored in the subtree with root x.ci, then:
  k1 <= x.key1 <= k2 <= x.key2 <= ... <= x.keyx.n <= kx.n+1.
4) All leaves have the same depth, which is the tree's height h.
5) Nodes have lower and upper bounds on the number of keys they can contain. We express these bounds in terms of a fixed integer t>=2 called the "minimum degree" of the B-tree:
  a) Every node other than the root must have >= t-1 keys. Every internal node other than the root thus has >=t children. If the tree is nonempty, the root must have >=1 key.
  b) Every node may contain at most 2t-1 keys. Therefore, an internal node may have at most 2t children. We say a node is "full" if it contains exactly 2t-1 keys. note: another viant called a B*-tree requires all nodes to be >= 2/3 full. 

The simplest B-tree occurs when t=2. Every internal node then has either 2,3, or 4 children (this is called a 2-3-4 tree). Generally t is much larger to reduce h, since the number of disk accesses for most operations on a B-tree is porprtional to h.
If n>=1, then for any n-key B-tree of height h and minimum degree t>=2:
  h <= logt((n+1)/2)  (that's log with base t)

Basic B-tree Operations
In our implementation of these methods, we assume the relevant satelite data (the reason the BTree exists is to hold this) travels around with the key (thus, a node holds multiple pieces of salelite data).
B-TREE-SEARCH(T.root,k)  // takes the root of a subtree, and a key to search for
 - Much like searching in a binary tree, except at each internal node x, we make an (x.n+1)-way branching decision.
 - Returns the node y and the index i such that y.keyi = k, or nil if no node is found to contain the key k.
B-TREE-CREATE()
B-TREE-INSERT
Inserting is more complex than inserting a key into a binary search tree, because rather than just creating a new node, we must insert the new key into an existing leaf node. We must also have a procedure for splitting full nodes that we encounter, since we can't insert new keys into full nodes. We split a node around its median k, x.keyt. 
The only way a tree grows is by node splitting. 
The only way to increase a B-tree's height is by splitting the root (this is different from a binary search tree, where height increases by adding new nodes at the bottom).

The above operations are all "one-pass" operations that proceed downward from the root of the tree, without having to back up.
We assume we always keep the root in main memory, so it does not require a disk read to access. However, it does require a disk write to update.

B-TREE-DELETE(T.root,k) // deletes the key k from the subtree rooted at x. Runs in time O(t logtn).
Deletion is analagous to insertion, but more complex because we can delete a key from an internal node or a leaf, whereas we only add to a leaf. When we delete from an internal node, we have to rearrange the children.
Just as we ensure we don't let a node get too big during insertion, we have to ensure it doesn't get too small during deletion (only the root can have fewer than t-1 keys). Just as the insertion algorithm may have to back up if a node on the path to where the key was to be inserted was full, a deletion algorithm may have to back up if a node (other than the root) along the path to where the key is to be deleted has the minimum number of keys.
We design the deletion procedure such that before it calls itself recursively on a node, we ensure that node has at least t (1 greater than the minimum) keys, so that one could theoretically be deleted.
See p. 500 for the rules.

-=-=-
Fibonacci-heap
2 purposes:
  1) supports a set of operations that constitutes what is known as a "mergable heap"
  2) several operations of a Fibonacci-heap run in constant amortized time, which makes the data structure well-suited for applications that invoke these operations frequently.

A mergable Heap is a data structure that supports the following 5 operations, in which each element has a key:
  MAKE-HEAP()  // creates and returns a new heap contining no empty elements
  INSERT(J, x)  // inserts element k, whose key has already been fulled in, into heap H
  MINIMUM(H)  // returns a pointer to the element in heap H whose key is minimum
  EXTRACT-MIN(H)  // deletes the element from heap H whose key is minimum, returning a pointer to the element
  UNION(H1, H2)  // creates and returns a new heap that contains all the elements of heaps H1 and H2 (H1 and H2 are destroyed).
Note: Default mergable heaps are min-heaps. We could also have mergable max-heap with operations MAXIMUM, EXTRACT-MAX, and INCREASE-KEY.
Fibonacci-heaps also support these 2 operations:
  DECREASE-KEY(H,x,k)  // assigns to element x within heap H the new key value k, which we assume to be no greater than its current key value.
  DELETE(H,k)  // delete element x from heap H

Running times for operations on 2 implementations of mergable heaps:
procedure           Binary Heap (worst case)        Fibonacci-heap (ammortized)
MAKE-HEAP             O(1)                            O(1)
INSERT                O(lg n)                         O(1)
MINIMUM               O(1)                            O(1)
EXTRACT-MIN           O(lg n)                         O(lg n)
UNION                 O(n)                            O(1)
DECREASE-KEY          O(lg n)                         O(1)
DELETE                O(lg n)                         O(lg n)

If we don't need the union procedure, ordinary binary heaps (as used in Heap Sort) work fairly well.
With binary heaps, the UNION operation works by concatinating the 2 arrays that represent the heaps, then running BUILD-MIN-HEAP. This takes O(n) worst case.
Neither binary nor Fib heaps perform SEARCH operations well; it's not really their purpose.

Fibonacci-heaps are especially desirable when the number of EXTRACT-MIN and DELETE calls are small relative to the number of other operations performed. This is particularly common in graph algorithms. 
However, Fibonacci-heaps are complex and contain a large number of constants. Thus, they're mainly used for theoretical purposes, or for applications that use large amounts of data.

A Fibonacci-heap is a collection of rooted trees. Each element is a node within a tree, and each node has a key. Each tree is "min-heap ordered". That is, each tree obeys the min-heap property: the key of a node is greater than or equal to the key of its parent.
Each node contains a pointer x.p to its parent and a single pointer x.child to any ONE of its children. 
The children of x are linked together in a circular, doubly linked list (the "child list" of x). 
Each child y in a child list has pointers y.left and y.right that point to its siblings. Sibling order is not important. 
Circular, doubly linked lists are handy because:
  - we can insert/remove a node into/from any lcation in the list.
  - we can concatenate 2 lists into 1 in O(1) time.
Each node has 2 more qualities:
  - We store the number of children in the child list of a node x as x.degree.
  - x.mark is a boolean that records whether x has lost a child since the last time x was made the child of another node. (Ff a marked node has a child removed, the node is moved to the root list and unmarked. This happens in DECRESE-KEY)
We access a Fib heap H by a pointer to the root of a tree containing the minimum key (we call this the "minimum node"). If the same key occurs in the root of another tree, either can serve as the minimum node. When H is empty, H.min is nil.
The roots of the trees are linked together via left/right in a circular doubly linked list called the "root list" of the heap. Trees may appear in any order.
H.n tracks the number of nodes currently in H.

Potential Function (used to perform ammortized analysis of runtime complexity)
P(H) = t(H) + 2m(H)  // t(H) is the number of trees in the root list of H, and m(H) is the number of "marked" nodes in H.

Operations:
INSERT  Adds a new node to the root list, which is constant time. If the new node is < H.min, we update H.min to point to the new node. All roots remain roots.
We avoid consolidating nodes in the root list into trees until we really have to (rather than doing it upon insertion). Otherwise, INSERT could not run in O(1) time.
MINIMUM  Simple: H.min points to it.
UNION  We simply concatenate the root lists of H1 and H2, and determine the new H.min. All roots remain roots.
EXTRACT-MIN  is the more complex, and it is also where the delayed work of consolidating trees in the root list finally occurs. 
Moves each child z of H.min to the root list and set z.p to nil, then removes H.min from the root list. If z is its own right child, set H.min to nil. Else, set H.min to z.right, which may not actually be the new min. Then calls CONSOLIDATE(H) helper function, which consolidates the root list bu repeatedly performing the following steps until every root in the root list has a distinct degree:
 1) Find 2 roots x and y in the root list with the same degree. Without loss of generality, let x.key <= y.key.
 2) Link y to x: trmove y from the root list, and make y a child of x using a helper FIN-HEAP-LINK, which increments the attribute x.degree and clears the mark on y.
A helper array A is used from [1,2,...,D(n)], which is a temp holder for root nodes of the root list as they are consolidated. The nodes are then taken from A and put back in H. There will be fewer than at the start.
Note that the maximum degree D(n) is O(lgn).
DECREASE-KEY
Decrease the value of node x. Call x.p y. If x.key < y.key, break its tie with y and move x up to the root list. If x is marked, unmark it. 
If y was not marked, mark it. If y was already marked, move it to the root list and unmark it. Do the same conditional operation on y.p until either the node's parent is nil or unmarked. The purpose of the mark (I believe) is just to help us find an upper bound when performing ammortized analysis.
If x is now < H.min, make it the new min.
DELETE  simply calls DECREASE-KEY on the node to delete to negative infinity, then calls EXTRACT-MIN :)

-=-=-
van Emde Boas Trees
We've seen several data structures that support the operations of a priority queue: binary heaps, red-black trees, and Fibonacci heaps. In the case of each structure, there is at least one operation that takes O(lg n) time, either worst case or amortized. Each of these structures bases its decision on comparing keys. The lower bound on search of O(nlgn) tells us the lower bound by definition: since you could sort by inserting each the keys into the structure, then extracting the min, you can only perform sorting in O(nlgn) if both the INSERT and EXTRACT-MIN operation take O(lg n) time (since you have to do them each n times to sort a list of length n).
But as we've seen (with counting sort, for example), you can exploit additional information to sort faster.
Analagously, van Embde Boas Trees support the priority queue operations, and a few others, each in O(lg lg n) worst case time using additional knowledge about the set of keys. The hitch is that the keys must be integers in the range  0 - n-1, with no duplicates.
In discussion below, we use n to denote the number of elements in the set, and u as the range of possible values (so each operation runs in O(lg lg u) time) where the highest value in the set is u-1. We assume for the sake of ease that u is an exact power of 2.
Operations: SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR. 

A vEB Tree is a recursive structure that stores integers using direct addressing. It has several properties:
 - u: The size of the universe of the data contained within the tree (if the tree can hold the values 0-3, then u=4).
 - min: the mininum integer value held in the tree. The element stored in V.min does not appear in any of the recursive trees that the cluster array points to.
 - max: the maximum integer value held in the tree. Unlike min, if V.u > 2, V.max does appear in a recursive subtree, unless V.max=V.min.
 - cluster: an array of length sqrt(u) that holds pointers to sqrt(u) other vEB trees, each having u=sqrt(u).
    If u=2, there's no need for this "cluster" array, since the values can be represented by "min" and "max".
 - summary: a pointer to another vEB tree of u=sqrt(u). This tree, rather than holding data about numbers in the set, holds data about whether each of the tree's clusters hold any data.
    Example: 
      If a tree V has V.u=16, it's V.summary points to a tree with u=4. 
      The summary tree has 2 sub-clusters, each of u=2. The first tells whether the 1st and 2nd clusters pointed to by V.clusters hold >=1 element. It does this via its min and max properties: if the 1st and 2nd clusters in V.clusters each hold >=1 element, min=0 and max=1. If only the first does, min=max=0. The second tells whether the 3rd and 4th clusters pointed to by V.clusters hold >=1 element, in the same way.
      The summary tree also has a summary prop, which points to another tree of u=2. This tree records whether the 2 cluster trees pointed to by the top level summary tree each hold >= 1 element, in the same way described above.

Why do the operations have a runtime of O(lg lg u), where u descrives the range of values that the tree can contain (the tree contains values of 0 - u-1)?
In a red-black tree, the height is ~lg n, which means search time is O(lg n) since at most you have to traverse through lg n nodes.
In vEB tree, the base tree has u=2, and u grows by a factor of u^2 with each level. For any value of u, we see that lg lg u = 2. Example: lg lg 16 = 2, since (2^2)^2 = 16. Thus, any operation requiring traversal has a runtime of O(lg lg u).
To make implementation easier, let's say u=2^(2^2k) for any integer k. While this limits our set of possible values for u to 2,3,16,256,65536, it ensures u is an integer. In practice, if we're willing to make use of some techniques that allow u to be a non integer, we can have u=2^k for any integer k.

Indexing
We need a way to index the elements of V in a way that's independent of U. 
To find the cluster number of V that an element x would be in, we use the 'high' function:
  high(x) = x / sqrt(u)
To find x's position within its cluster, we use the 'low' function:
  low(x) = x % sqrt(u)
To build an element number from x and y, we use the 'index' function:
  index(x,y) = x*sqrt(u) + y


-=-=-
Data Structures for Disjoint Sets (sets that have no elements in common)
Some applications involve grouping n distinct elements into a collection of disjoint sets. These applications often need to perform 2 operations in particular:
  1) finding the unique set that contains a given element
  2) uniting 2 sets
This section explores methods for maintinaing a data structure that supports these operations.
A disjoint-set data structure maintains a collection S={S1, S2, ..., Sk} of disjoint dynamic sets. We identify each set by a representative, which is some member of the set. Sometimes it doesn't matter which member is used, as long as it doesn't change as long as the set is unchanged. Sometimes it needs to be a specific member, such as the minimum (assumng the set elements can be ordered).


Disjoint set operations (letting x denote a Set object):
  MAKE_SET(x)  creates set whose only member (and thus representative) is x. Since the sets are disjoint, we require that x not already exist in some other set.
  UNION(x,y)  unites the dynamic sets that contain x and y (call them Sx and Sy) into a new set that is the union of these 2 sets. Sx and Sy are deleted (though really we may move 1 into the other). The new representative is any element in the new set, but is often one that was a representative of Sx or Sy.
  FIND-SET(x)  returns a pointer to the representative of the (unique) set containing x.
One application of such a data structure would be determining whether nodes are part of the same undirected, connected graph. (See 21-1 for example)

In this discussion, we analyze the running times of disjoint data-structures in terms of 2 parameters:
 - n: the number of MAKE-SET operations
 - m: the number of MAKE-SET, UNION, and FIND-SET operations.
Each union operations reduces the number of sets by 1. The number of union operations is capped at n-1, since after n-1 unions, only 1 set remains.
Since MAKE-SET operations are included in the total number of operations m, we have m >= n.
We assume that the n MAKE-SET operations are the first n operations performed.

-=- Linked List Implementation
Each set represented by its own linked list.
The object for each set has the attributes head (pointing to first object in the list) and tail (pointing to the last object).
Each object in the list contains a set member, a pointer to the next object in the list, and a pointer back to the set object.
Order does not matter within the list. The representative is the first object in the list.
With this implementation, MAKE-SET and FIND-SET are easy and have O(1) runtime.
MAKE-SET(x) just creates a new linked list whose only object is x.
FIND-SET(x) simply follows the pointer from x back to the set object.
UNION(x,y) takes more time. It appends y's list to the end of x's list and deletes y's list object. The representative of x's list becomes the rep for the new list. We must update the pointer on each member of Sy to point to the new set. 
Runtime of UNION:
  Since we need to update the pointers, a series of m MAKE-SET operations followed by a series of m-1 UNION operations actually takes O(n^2) time.
  However, if we were to track the length of each linked list (easy  to do) and ensure the shorter list is always appended to the longer, the runtime of the above operation is O(n lg n).

-=- Disjoint Set Forests
A faster way is to represent sets as rooted trees, with each node representing one member and each tree representing one set. Each member (node) points only to its parent. The root node points to itself.
By introducing 2 heristics, we can acheive an asymptotically optimal disjoint set data structure:
  - "union-by-rank" (done during UNION(x,y)) and
  - "path-compression" (done during FIND-SET(x))

Operations:
MAKE-SET(x) creates a tree with only a root node x.
FIND-SET(x) follows x's parent chain until it reaches the root. The nodes visited are called the "find path".
UNION causes the root of one tree to point to the root of another. Unlike the linked list implementation, this implementation is just barely superlinear. Practically, it is O(n).

We need to make sure the UNION operations don't produce just a linear chain of nodes. The 2 heristics help us:
 - Union by rank. For each node, we maintain a "rank", which is an upper bound on the height of the node (the number of edges in the longest simple path from a descendant leaf to x). In "union by rank", we make the root with smaller rank point to the root with the larger rank during a UNION operation.
 - Path compression. Make each node on the "find path" point directly to the root. Path compression does not change any ranks.

Pseudocode for operations:
MAKE-SET(x)
  x.p = x
  x.rank = 0

LINK(x, y)  // helper used by UNION
  if x.rank > y.rank
    y.p = x  // ranks remain unchanged
  else
    x.p = y
    if x.rank == y.rank  // arbitrarily chose y as the parent. Increment its rank by 1
      y.rank++

UNION(x, y)
  LINK(FIND-SET(x), FIND-SET(y))

FIND-SET(x)  // FIND-SET is a 2-pass method. Makes a pass up to find the root, then makes a pass back down the find path to update each node to point directly to the root. Path compression does not change any ranks.
  if x != x.p
    x.p = FIND-SET(x.p)
  return x.p