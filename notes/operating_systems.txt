Concurrency: Having multiple tasks that can be worked on at the same time, but not necessarily working on them at the same time. For example, a grocery list. Concurrency is a prerequisite for parallel execution, since you need tasks that can be done simultaneously.
Parallelism: The ability to actually work on those concurrent tasks at the same time. For example, you and your friends splitting up to grab everything on the grocery list.

-=- Process
The instance of a computer program that is being executed by one or many threads that execute instructions concurrently. 
To be run, a program is compiled down to binary (the only form of instruction the CPU understands), and is loaded into memory along with resources it needs to run as a process:
 - registers: places that are part of the CPU where data is held. Registers can hold an instruction, storage address, or other data needed by the process.
 - a program counter/instruction pointer: keeps track of where a computer is in its program sequence.
 - a stack: data structure that stores information about the active subroutines of a program, and is used as a scratch space for a process.
 - heap: dynamically allocated memory for the process (different from the stack).

Each process has a separate memory address space, which makes it isolated from other processes. 
"Multitasking" allows processes to share processors (CPUs) and other system resources.
While each CPU (core) executes a single task at a time, multitasking allows each processor to switch between tasks that are being executed without having to wait for each task to finish.

-=- Thread (of execution)
The smallest instance of programmed instructions that can be managed independently by a scheduler, which is typically part of an OS.
On most OS's, a thread is a component of (and a unit of execution within) a process.
Multiple threads can exist within 1 process, executing concurrently and sharing resources such as memory, while different processes do not share these resources.
In particular, threads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local variables at any given time.
 - Each thread has its own memory and execution stack (so do goroutines), but share the heap allocated to the process. 
 - A thread usually has ~1Mb allocated to it (goroutines have a variable stack space that starts at ~2kb, and can grow from there. More efficient memory usage).
 - Threads are managed by the OS (goroutines are managed by the Go runtime - under the hood, it schedules those goroutines onto threads. Typically there are more goroutines than threads. There can be hundreds of thousands of goroutines active at the same time, but not that many threads).

Threads vs processes
 - processes are typically independent, whereas threads exist as subsets of a process
 - processes cary more state than threads, whereas multiple threads within a process share process state as well as memory (heap) & other resources.
 - processes have separate adress spaces, whereas threads share the same adress space of their process.
 - processes only interact through system-provided inter-process communication mechanisms.
 - context switching between threads is typically faster than context switching between processes.

Single threading
processing 1 command at a time.

Multithreading
Allows multiple threads to exist within 1 process.
The threads share the processes' resources, but operate independently. Allows concurrent execution.
Multithreading can also be applied to 1 process to enable parallel execution on a multiprocessing system.
Advantages:
 - responsiveness: allows a process to respond to input even while executing tasks in the background.
 - faster execution on machines with multiple CPUs due to parallel execution.
 - lower resource consumption: can serve multiple clients concurrently using multiple threads, which uses fewer resources than running multiple processes.
 - better system utilization: for example, 2 threads within 1 process accessing different areas of storage (one disk, one memory) in parallel.
 - simplified sharing & communication: unlike processes, threads within a process can communicate easily.
Disadvantages:
 - synchronization: race conditions, deadlocks, livelocks.
 - trying to perform an illegal operation in a thread can crash the entire process. 

Context Switching
The process of storing the state of a process or thread, so that it can be resumed later. This allows multiple processes to share a single CPU.
Computationally expensive. Saving and loading registers and memory maps. Switching between threads is faster, because threads share the same virtual memory maps. 
Some processors have hardware support for context-switching, which sounds faster, but windows and linux do not use this feature due to complications.
3 potential triggers for a context switch:
  1) Multitasking: one process being switched out of the CPU so another can run. This can be triggered by:
    - a process making itself unrunnable, such as when waiting for an I/O or synchronization operation to complete.
    - a preemptive scheduler triggering a timer interrupt when a process exceeds its time slice.
  2) Interrupt handling: if the CPU requests data from disk or an HTTP request (for example), rather than busy-waiting until a response is received, it can issue the request and continue on with some other execution. When the data is fetched, the CPU can be interrupted and presented with the data, so the initial process or thread can continue.
  3) User and kernel mode switching. This only requires a context switch on some OS's.
Steps:
 - State of the current process must be stored, so it can be resumed. Process state includes any registers it is using, memory heaps, the program counter, and other OS-specific data. This process state is stored in a data structure called "process control block" (PCB), which is stored on a per-process stack in kernel memory.
 - A handle to the PCB is added to the queue of processes that are ready to run (the "ready queue").
 - The OS can then select another process from the ready queue and restore its PCB. Process and thread priority can influence which process is chosen from the ready queue.

-=- Scheduling
The method by which work is assigned to resources that complete the work.
Schedulers are generally implemented so they keep all resources busy. 
Scheduling makes it possible to have multitasking with a single CPU.
A scheduler may have multiple priorities, some of which sometimes conflict:
 - maximize throughput (total work done per time unit)
 - minimize wait time (time between a task being ready and execution beginning)
 - minimize latency (time between a task being ready and being complete)
 - maximize fairness (CPU time given to each process)
2 Types of process schedulers:
 - Preemptive Scheduling: common on multi-user machines for its finer grained control over execution time via context switching. However, may result in context switches at unanticipated moments.
 - Cooperative scheduling: requires the thread to relinquish control of execution, thus ensuring that threads run to completion. Can create problems if a thread blocks while waiting on a resource.

-=- Lock
A synchronization mechanism for enforcing limits on access to a resource in an environment where there are many threads of execution. A lock is designed to enforce a "mutual exclusion concurrency control" policy.
Generally, locks are "advisory locks", where each thread cooperates by acquiring the lock before accessing the corresponding data. 
Some systems also implement "mandatory locks", where attempting unauthorized access to a locked resource will force an exception in the entity attempting to make the access.

Lock Types
The simplest type of lock is a "semaphore", which provides exlcusive access to the locked data. Other schemes provide shared access for reading data. Other modes include "exclusive", "intend-to-exlcude", and "intend-to-upgrade".
Another way to classify locks is by what happens when the lock strategy prevents progress of a thread. Most locking designs block the execution of a thread requesting the lock until it is allowed to access the locked resource (for example, within a "spinlock", the thread simply waits until the lock becomes available). This is efficient if threads are blocked for a short time as it avoids the overhead of rescheduling, but is inefficient for long lock times. 

Locks typically require hardware support for efficient implementation. This support usually takes the form of one or more atomic instructions such as "test-and-set", "fetch-and-add" or "compare-and-swap". These instructions allow a single process to test if the lock is free, and if free, acquire the lock in a single atomic operation.
An atomic operation is required because of concurrency, where more than 1 process executes the same logic. For example:
  if (lock == 0) {
    lock = myPID;
  }
The above code does not guarantee that the task has the lock, since >1 tasks may be testing the lock simultaneously.
Careless use of locks can result in livelock or deadlock. To avoid these, the most common strategy is to standardize the lock aquisition sequences so that combinations of inter-dependent locks are always acquired in a specifically defined "cascade" order.

Lock Granularity
Concepts:
 - lock overhead: the extra CPU and memory space required for managing locks
 - lock contention: occurs when one process or thread attempts to acquire a lock held by another process of thread.
There's a tradeoff between keeping lock overhead and lock contention low when choosing how many locks to use in synchronization.
Granularity is the amount of data protected by a lock. Low granularity -> low overhead but high contention, since the courser the lock, the more likelihood it will block an unrelated process. 
Granular locks, where each process must hold multiple locks from a common set can potentially cause deadlock.

-=- Mutex (from "mutual exclusion") (seems to be used synonymously with "lock")
Mutual exclusion is a property of concurrency control, used to prevent race conditions.
It is the requirement that 1 thread of execution never enters its critical section (the interval of time during which it accesses a shared resource, such as shared memory) at the same time that another concurrent thread enters its own critical section. 
Must be implemented in such a way as to not cause deadlocks.
A common implementation is called "busy-waiting". This uses shared memory and an atomic "test-and-set" instruction. A process can "test-and-set" on a location in shared memory, and since the operation is atomic, only one process can set the flag at a time. Any process that is unsuccessful in setting the flag can either go on to do other tasks and try again later, release the processor to another process and try again later, or continue to loop while checking the flag until it is successful in acquiring it.

Mutex vs generic "Lock"
 - A lock allows only one thread to enter the part that's locked and the lock is not shared with any other processes.
 - A mutex is the same as a lock but it can be system wide (shared by multiple processes).

-=- Deadlock
The situation when each of >= 2 tasks is waiting for a lock that the other task holds (stuck forever).
Can only occur if all the following conditions hold simultaneously:
 - mutual exclusion: at least 1 resource must be held in a non-sharable mode. Only 1 process/thread can use the resource at any given time.
 - "hold and wait" or "resource holding": a process is currently holding 1 resource and requesting additional resource(s) being held by other processes/threads.
 - no preemtion*: a resource can be released only voluntarily by the process holding it.
 - circular wait: each task must be waiting for a resource that is being held by another task, which is in turn waiting for the first task to release the resource it is holding.

*preemtion is the act of temporarily interrupting a task without requiring its cooperation, and with the intent of resuming the task later. Such interruptions are known as context switches, and are normally carried out by a privileged task of part of the system known as a "preemptive scheduler".

-=- Livelock
- Similar to deadlock, except the processes involved constantly change with regard to one another, none progressing.
Example: 2 people meet in a corridor, and keep moving side-to-side to try to make room for the other.
- Can occur when we try to avoid deadlock using asynchronous locking, where multiple threads compete for the same set lock(s), avoid acquiring them to allow other threads to take the lock first, and never actually get to proceed. This causes resource starvation and gets nothing done.
- Livelock is a risk with some algorithms that detect and recover from deadlock. If >1 process takes action, the deadlock detection algorithm can be repeatedly retriggered. This can be avoided by ensuring that only 1 process (chosen arbitrarily or by priority) takes action.

-=- Semaphores
Variable or abstract data type used to control access to a common resource by multiple processes on a concurrent system. 
2 types:
 1) counting semaphore: records how many units of a particlular resource are available (for example, messages or free spaces in a queue). Counting queues come with operations P (decrements the count) and V (increments the count). It is crucial that P and V are atomic. This can be accomplished either via a built-in machine instruction, or using a "mutual exclusion algorithm" in the code.
 2) binary semaphores are restricted to the values 0 and 1 (locked/unlocked).
A semaphore is coupled with operations to adjust the record safely (avoiding race conditions) as units become free or are acquired, and if necessary, to wait until a unit becomes available.
Counting Semaphore Methods:
 - Wait (P): decrements the value of the semaphore variable by 1. If the new value is negative, the process/thread is blocked, and added to the semaphore's queue. Otherwise it continues, having used a unit of the resource.
 - Signal (V): Incrememnts the value of the semaphore by 1. If the pre-increment value was negative - meaning there are processes/threads waiting for a resource - it transfers a thread from the semaphore's waiting queue to the ready queue.
Semaphore count may serve as a trigger for a number of different actions. For example, shutting off lights when nobody is in a room.
Many OS's provide efficient semaphore primitives to block/unblock processes, so they don't waste resources checking the semaphore's value.

If a process does any of these things, the semaphore will not work as desired:
 - request a resource and forget to release it
 - release a resource that was never requested
 - holding a resource for a long time without needing it
 - using a resource without requesting it (or after releasing it)
Even if rules are followed, multi-resource deadlock may still occur when there are different resources managed by different semaphores and when processes require multiple resources simultaneously.
Example of using a counting semaphore is a queue implementation with producer and consumers. The queue has a max length of n. 3 semaphores are required: 
    - fullCount: used by consumers so they know when they can extract a message (<= the number of messages in queue)
    - emptyCount: used by producers so they know when they can add a message (<= the number of empty spaces, used by producers)
    - useQueue: says whether or not the queue is locked. 

Semaphores vs Mutexes:
A mutex is like a binary semaphore, but the difference is in how they are used. A true mutex can only be unlocked by the task that locked it. This constraint aims to handle some potential problems with semaphores:
 - priority inversion: if the mutex knows who locked it (and thus is supposed to unlock it), it's possible to promote the priority of that task whenever a higher pri task comes along.
 - termination deadlock: the OS can release the mutex if the task prematurely terminates.
 - an error can be raised on release of the mutex if the releasing task is not the owner.

-=- Monitors
A monitor is a synchronization construct that allows threads to have both mutual exclusion, and the ability to wait (block) for a certain condition to become false.
 - A monitor consists of a mutex (lock) object, and "condition variables". A condition variable is basically a container of threads that are all waiting for a certain condition. 
 - We often have a one-to-many relationship between mutexes and condition variables. That is, condition variables often use the same mutex (for example, queueEmpty and queueFull condition variables using the same queueLock). But a single condition variable won't use multiple mutexes.
 - Using mutexes and condition variables, monitors provide a way for a thread to give up exlcusive access (the mutex) in order to wait for some condition to be met, then regain access and resume its task. All without wasting CPU resources by busy-waiting.
 - Locks and condition variables can be derived from semaphores. Thus, monitors and semaphores are reducible to one another.
 - Another definition of "monitor" could be a threadsafe class that wraps around a mutex to safely allow access to a method or variable by >1 thread.

Condition variables
While a thread is waiting on a condition variable, that thread does not hold the mutex (occupy the monitor), and thus other threads can enter the monitor and change its state.
3 operations supported by condition variables:
 - WAIT(c, m) where c is the condition variable Pc and m is the mutex associated with the monitor. Does the following:
  Atomically:
    a) release the mutex (was just previously acquired)
    b) move this thread from 'running' to c's wait queue.
    c) sleep this thread (context is yielded to another thread). (This thread is then subsequently notified/signalled and resumed later, at which point it automatically reacquires the mutex)
    The atomicity of these opreations is important to avoid race conditions that would be caused by a preemptive thread switch in-between them. For example: if another thread calls "signal" before this thread goes to sleep, this thread will then go to sleep and never wake up.
 - SIGNAL(c): indicate that Pc is now true, and moves one or more threads in c's "sleep queue" to a "ready queue" for execution. 
 - BROADCAST(c) wakes up all threads in c's wait queue.

Example of a simple queue with Producer and Consumer threads:
// create global vars
RingBuffer queue // thread-unsafe ring-buffer of tasks
Lock queueLock // mutex for the queue
CV queueEmptyCV  // condition var for consumer threads waiting for the queue to be non-empty. Associated lock is queueLock.
CV queueFullCV  // condition var for producer threads waiting for the queue to be non-full. Associated lock is also queueLock.

// Method representing each producer thread's behavior:
Producer() {
  while (true) { // infinite loop to signify an ever-running thread
    task myTask = ...' // create new task
    queueLock.acquire(); // aquire lock for conditional predicate check
    while (queue.isFull()) {  // do not proceed past this loop unless queue is not full
      // make threading system automatically release queueLock
      // enqueue thread into queueFullCV, then sleep thread.
      wait(queueLock, queueFullCV);
      // "wait" automatically reacquires queueLock when "signal" is called by a Consumer thread,
      // at which point we re-check the predicate condition (the queue being full)
    }
    // Critical section that requires the queue to be non-full. We are holding queueLock.
    queue.enqueue(myTask); // Add the task to the queue.

    // Now the queue is guaranteed to be non-empty, so signal a consumer thread
    // or all consumer threads that might be blocked waiting for the queue to be non-empty:
    signal(queueEmptyCV); -- OR -- notifyAll(queueEmptyCV);

    // End of critical sections related to the queue.
    queueLock.release(); // Drop the queue lock until we need it again to add the next task.
  }
}

"Consumer" implementation is symmetric to "Producer".
